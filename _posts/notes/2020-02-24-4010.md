---
class_code: stat4010
---
# Regression Diagnostics

In previous units, we have often taken for granted that the MLR assumptions have been met. In this unit, we consider techniques (both graphical and numerical) for diagnosing deviations fron these assumptions and potential solutions.

__Note:__ Formal (numerical) tests may seem more "objetive -- incomplete

## Constant Variance Assumption

Recall one of our assumptions is $var(\epsilon_i) = \sigma^2$, $\forall i = 1,\dots,n$. Alternatively $var(\underline{\epsilon}) = \sigma^2 I_n$. This also implies independence, because $\sigma^2 I_n$ is diagonal.

Even under this assumption, the residuals - out estimator of the error - are not constant variance:

\begin{align}
\hat{\epsilon} &= \underline Y - \underline{\hat{Y}} \\\ 
&= \underline Y - X\underline{\hat{\beta}} \\\ 
&= \underline Y - X(X^TX)^{-1}X^T\underline Y \\\ 
&= \underline Y - H\underline Y \\\ 
&= (I_n - H)\underline Y
\end{align}

where $H = X(X^TX)^{-1}X^T$ is the "hat matrix".

$\underline Y$ is random, but $I_n - H$ is not. So we can use this for variance:

\begin{align}
var(\underline{\hat\epsilon}) &= cov(\underline{\hat\epsilon},\underline{\hat\epsilon}) \\\ 
&= cov((I_n-H)\underline Y, (I_n-H)\underline Y) \\\ 
&= (I_n-H)cov(\underline Y, \underline Y)(I_n-H)^T \\\ 
&= (I_n-H)\sigma^2I_n(I_n-H)^T \\\ 
&= \sigma^2(I_n-H)(I_n-H) \text{ because $I_n$ and $H$ are symmetric} \\\ 
&= \sigma^2(I_n-H)
\end{align}

Using the property $cov(Ay,By) = A\cdot cov(y,y)\cdot B^T$. Note that since the diagonal terms are not the same, the residuals are not constant variance. Also there is a non-zero covariance in the residuals in general, because the off diagonals are non-zero.

Fortunately, the differences in variance of the residuals is typically small, and we can use the residuals to check this assumption.

1. __Residual vs Fitted Values Plot:__ If the model fits well, you should see constant symmetrical variation in this plot.
2. __Residual vs Potential Predictor Plot:__ Any observed structure here indicates that the predictor should be in the model

Sample code for generating data with non-constant variance:

```R
x = seq(-1,1,length.out=n)
y = beta_0+beta_1*x+rnorm(n,0,sd=sigma)*X
```