---
class_code: stat4010
---
# Checking Assumptions

## Solutions to Non-Constant Variance

1. _Transformation of the response_. FOr example, if your response is known to be Poisson (it's a count), its variance and mean will be related. A square root transformation might stabilize the variance.
2. _Weighted/Generalized Least Squares_. If the structure of the model is correct, we might perform weighted least squares, which allows us to correct for unequal variance (and generalized LS allows us to correct for correlation).

For weighted least squares: If $\hat\beta = (X^TX)^{-1}X^T\underline Y$, then $\hat{\beta_{wls}} = (X^TWX)^{-1}X^TW\underline Y$, where:

\\[W = \begin{pmatrix} w_1&\dots&0 \\\ \vdots&\ddots&\vdots \\\ 0 &\dots& w_n\end{pmatrix}\\]

```R
lmod = lm(..., weights=W)
```

## Diagnosing Correlated Errors

This might happen with spatial or temporal data.

In general, it is difficult to check for correlated errors because there are too many possible correlation patterns. But in some special cases, correlation patterns might be relatively simple. We might look at:

1. _Residuals vs Time/Index:_ In these plots, we're looking for sequences of points (clusters) above and below the line $y=0$.
2. _Successive residual plot:_ We could plot $\hat\epsilon_i$ against $\hat\epsilon_{i+1}$ and look for a correlation. This helps with successive correlation but won't help with a more complicated correlation structure.
3. _Durbin-Watson test:_ This is a formal test whether successive errors are correlated. Let $\epsilon_{i+1} = \rho\epsilon_i+\gamma$, where $\gamma\sim N\left(0, \left(\sigma^\*\right)^2\right)$. The null hypothesis is $H_0: \rho = 0$. The test statistic in practice is a little complicated.

## Solutions to Correlated Errors

1. _Generalized Least Squares:_ If you know the correlation structure, you could conduct GLS. $\hat\beta_{GLS} = (X^T\Sigma^{-1}X)^{-1}X^T\Sigma^{-1}\underline Y$, where $\Sigma^{-1} = var(\underline\epsilon)$.
2. _Time Series/Spatial Statistics:_ We have entire courses dedicated to these topics! (And exploring some methods as they related to regression would make for an interesting project.)

## Disagnosing Non-Normality

Recall that inference and prediction required an assumption that the errors were normally distributed. To diagnose deviations from normality, we can look at:

1. _QQ Plots:_ This plots compare the quantiles of the residuals to the theorectical quantiles of the standard normal distribution. We hope to see quantile points fall along a straight diagonal line.
2. _Shapiro-Wilk Test:_ This is a formal test for normality. $H_0$ is that the residuals are normal. Note that this test is sensitive to small deviations from normality when the sample size is large. Friends don't let friends $p$-hack with large datasets.

## Solutions to Non-Normality

Soon, with out study of _generalized linear models_ (GLMs), we will see that we can relax the normality assumption and allow for our response to come from any distribution from the _exponential family of distributions_.